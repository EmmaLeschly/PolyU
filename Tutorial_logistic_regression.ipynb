{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZQ6SV2Zg-9vi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import seaborn as sns\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbDIkJcX-9vj"
      },
      "outputs": [],
      "source": [
        "sns.set(color_codes=True)\n",
        "random.seed(1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBOYLWje-9vj"
      },
      "source": [
        "## 1. The training dataset generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmSH2DL6-9vk"
      },
      "source": [
        "Synthesized data, which is generated as 2-D  smaples according to two different Gaussian distributions, will be used.\n",
        "\n",
        "Therefore, we need first define the mean and covariance of each Gaussian distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VcIMfcE-9vk"
      },
      "outputs": [],
      "source": [
        "nums_train = 200 # set the number of the training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iWKKjz7-9vk"
      },
      "outputs": [],
      "source": [
        "train_m1 = np.array([1, -1]) ## the mean of the 1-st Gaussian distribution\n",
        "train_m2 = np.array([-1, 1])  ## the mean of the 2-nd Gaussian distribution\n",
        "train_means = [train_m1, train_m2]\n",
        "\n",
        "print(\"The means of the two Gaussian distributions:\", train_means[0], train_means[1])\n",
        "\n",
        "## define the two covariance matrices of the two Gaussian distributions\n",
        "train_c1 = np.array([[1,0], [0,1]])\n",
        "train_c2 = np.array([[1, 0], [0, 1]])\n",
        "train_covs = [train_c1, train_c2] ## Each covariance matrix decides the shape of the Gaussian distribution\n",
        "\n",
        "print(\"The covariance matrices of the two Gaussian distributions:\\n\", train_covs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBlc9hGA-9vl"
      },
      "outputs": [],
      "source": [
        "def sample_generation(means, covs, nums):\n",
        "    \n",
        "    ''''\n",
        "    input:\n",
        "        - means: a list with 2 elements, and each represents the mean of a Gaussian distribution\n",
        "        - covs: a list with 2 elements, and each represents the covariance matrix of a Gaussian distribution\n",
        "        - nums: a scalar, the number of data points to be generated\n",
        "    \n",
        "    output: an array with the shape of (nums, 3)\n",
        "    '''\n",
        "    \n",
        "    # sampling (nums//2) points from the 1st Gaussian distribution with means[0] and covs[0]\n",
        "    x_1 = np.random.multivariate_normal(means[0], covs[0], nums//2)\n",
        "    y_1 = np.zeros((nums//2, 1))\n",
        "    \n",
        "    # sampling (nums//2) points from the 2nd Gaussian distribution with mean[1] and covs[1] \n",
        "    x_2 = np.random.multivariate_normal(means[1], covs[1], nums//2)\n",
        "    y_2 = np.ones((nums//2, 1))\n",
        "    \n",
        "    # In this demo, we define the shape of our data as (nums, 3)\n",
        "    # The first two columns represent features, the 3rd column is the label.\n",
        "    \n",
        "    d1 = np.hstack((x_1, y_1))\n",
        "    d2 = np.hstack((x_2, y_2))\n",
        "    data = np.vstack((d1, d2))\n",
        "    \n",
        "    # Shuffle the data, or rearrange the order of the data\n",
        "    index = [i for i in range(0, nums)]\n",
        "    random.shuffle(index)\n",
        "    data = data[index,:]\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mB7r0Bof-9vm"
      },
      "outputs": [],
      "source": [
        "train_data = sample_generation(train_means, train_covs, nums_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNQ9AVcI-9vm"
      },
      "source": [
        "#  data visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjABSgfK-9vm"
      },
      "outputs": [],
      "source": [
        "def data_visual(data):\n",
        "    \n",
        "    label_set = set(data[:,2].flatten())\n",
        "    signs = ['r*', 'bo']\n",
        "    \n",
        "    for i,label in enumerate(zip(label_set)):\n",
        "        index = np.where(data[:,2] == label)\n",
        "        x = data[index, 0]\n",
        "        y = data[index, 1]\n",
        "        plt.plot(x, y, signs[i])\n",
        "    \n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_upquDg-9vn"
      },
      "outputs": [],
      "source": [
        "data_visual(train_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXhP-mNq-9vn"
      },
      "source": [
        "## Logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3aj__AH-9vn"
      },
      "source": [
        "### Sigmoid function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHFEHE2P-9vo"
      },
      "source": [
        "$$ h_\\theta(x) = \\frac{1}{1+ e^{-\\theta^T{x}} } $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7H9uQV8-9vo"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    \n",
        "    '''\n",
        "    input:\n",
        "        - X: an array with the shape of (nums, 1)\n",
        "    \n",
        "    output: an array with the shape of (nums, 1)\n",
        "    '''\n",
        "    y = 1/(1 + np.exp(-x))\n",
        "    \n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r0Ey69K_-9vo"
      },
      "outputs": [],
      "source": [
        "def h(X, theta):\n",
        "    \n",
        "    '''\n",
        "    input:\n",
        "        - X: the training data array  with the shape of (nums, 2)\n",
        "        - theta: the weight array with the shape of (2, 1)\n",
        "    \n",
        "    output: an array with the shape of (nums, 1)\n",
        "    '''\n",
        "    return sigmoid(np.dot(X, theta))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfHHf-AJ-9vo"
      },
      "source": [
        "### Logistic Regression Cost Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1VE5eIL-9vo"
      },
      "source": [
        "$$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [-y^{(i)}\\log (h_\\theta(x^{(i)})) - (1-y^{(i)})\\log (1-h_\\theta(x^{(i)})]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-oF1598-9vp"
      },
      "source": [
        "### Gradient descent update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaHEZhTq-9vp"
      },
      "source": [
        "$$ \\theta_j :=  \\theta_j  - \\alpha \\sum_{i=1}^{m} h_\\theta(x^{(i)} - y^{(i)}) x_j^{(i)} $$\n",
        "    (simultaneously update all $\\theta_j$)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKXW1mNU-9vp"
      },
      "outputs": [],
      "source": [
        "def object_fun(X, theta, y):\n",
        "    '''\n",
        "    input:\n",
        "        - X: an array with the shape of (nums, 2), nums means the number of sample points, and 2 represents\n",
        "            the dimension of the features.\n",
        "        - theta: (2, 1) array.\n",
        "        - y: an array with the shape of (nums, 1), containing labels of the sample points.\n",
        "    \n",
        "    output: a scalar\n",
        "    '''\n",
        "    \n",
        "    nums_sample = X.shape[0]\n",
        "    h_theta = h(X, theta)\n",
        "    L = (1.0 / nums_sample) * ((-y).T.dot(np.log(h_theta)) - (1.0 - y.T).dot(np.log(1.0-h_theta)))\n",
        "    grad = (1.0/nums_sample) * X.T.dot(h_theta - y)\n",
        "    return L, grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt2InEIv-9vp"
      },
      "outputs": [],
      "source": [
        "def predict(X, theta):\n",
        "    '''\n",
        "    input:\n",
        "        - X: an array with the shape of (nums, 2), nums means the number of sample points, and 2 represents\n",
        "            the dimension of the features.\n",
        "        - theta: (2, 1) array.\n",
        "    \n",
        "    output: a binary arrary with the shape of (nums, 1)\n",
        "    '''\n",
        "    \n",
        "    # Perform sigmoid function first\n",
        "    pred = h(X, theta)\n",
        "    \n",
        "    # The ouput of the sigmoid function is continuous, a threshold should be used for the classification task.\n",
        "    # In this example, the threshold is set to 0.5.\n",
        "    \n",
        "    pred[pred >= 0.5] = 1 # Output values above 0.5 are set to 1ã€‚ \n",
        "    pred[pred < 0.5] = 0 # Output values above 0.5 are set to 0.\n",
        "    return pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c496u2xR-9vq"
      },
      "outputs": [],
      "source": [
        "def comp_accuracy(X, theta, y):\n",
        "    '''\n",
        "    input:\n",
        "        - X: an array with the shape of (nums, 2), nums means the number of sample points, and 2 represents\n",
        "            the dimension of features.\n",
        "        - theta: (2, 1) array.\n",
        "        - y: an array with the shape of (nums, 1), containing labels of sample points.\n",
        "    \n",
        "    output: a scalar\n",
        "    '''\n",
        "    \n",
        "    pred = predict(X, theta)\n",
        "    \n",
        "    return np.mean(pred==y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-crw3C1-9vq"
      },
      "outputs": [],
      "source": [
        "## training data preparation\n",
        "train_x, train_y = train_data[:, 0:2], train_data[:,2]\n",
        "train_y = train_y.reshape(nums_train, 1)\n",
        "\n",
        "## paramter settings\n",
        "iterations = 1000  ## set the number of iterations for gradient descent\n",
        "theta_init = np.random.rand(2,1)  ## set the initial value of the theta.\n",
        "\n",
        "theta_old = theta_init\n",
        "alpha = 0.005 ## the learning rate\n",
        "loss_list = [] ## a list to record the loss at each iteration\n",
        "acc_list = [] ## a list to record the accuracy at each iteration\n",
        "\n",
        "for ite in range(0, iterations):\n",
        "    \n",
        "    ## compute the loss\n",
        "    loss, grad = object_fun(train_x, theta_old, train_y)\n",
        "    loss_list.append(loss[0,0])\n",
        "    \n",
        "    ## update theta\n",
        "    theta_new = theta_old - alpha * grad\n",
        "    \n",
        "    ## compute the current accuracy\n",
        "    acc_temp = comp_accuracy(train_x, theta_old, train_y)\n",
        "    acc_list.append(acc_temp)\n",
        "    \n",
        "    theta_old = theta_new\n",
        "    if (ite+1) % 100 == 0:\n",
        "        print(\"The iteration {}, the loss is {:.4f}, the accuracy is {:.4f}\".format((ite+1), loss[0,0], acc_temp))\n",
        "\n",
        "theta_hat = theta_old  ## final updated paramters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nDEd9qa-9vq"
      },
      "source": [
        " ## Visualization for loss and accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix_qNTv9-9vr"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(loss_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"loss\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(acc_list)\n",
        "plt.xlabel(\"iteration\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.ylim((0,1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2YdAVqU-9vr"
      },
      "source": [
        "## Visualization for the decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znmrvdNn-9vr"
      },
      "outputs": [],
      "source": [
        "ss = 0.02  ## step size\n",
        "\n",
        "# create a mesh to plot in\n",
        "x_min, x_max = train_x[:, 0].min() - 1, train_x[:, 0].max() + 1\n",
        "y_min, y_max = train_x[:, 1].min() - 1, train_x[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, ss),\n",
        "                     np.arange(y_min, y_max, ss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD7Lhqof-9vr"
      },
      "outputs": [],
      "source": [
        "# Plot the decision boundary. For visualization, we assign a color to each\n",
        "# point in the mesh [x_min, x_max]x[y_min, y_max].\n",
        "temp = np.c_[xx.ravel(), yy.ravel()]\n",
        "pred = predict(temp, theta_hat)\n",
        "Z = pred.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=0.6)\n",
        "label_set = set(train_y.flatten())\n",
        "\n",
        "# set the color we would like to assign to each point.\n",
        "color_map = {1: (0, 0, .9), 0: (0.9, 0, 0)}\n",
        "color = [color_map[i] for i in list(train_y.ravel())]\n",
        "color = np.asarray(color)\n",
        "\n",
        "# Plot the training points\n",
        "for i,label in enumerate(zip(label_set)):\n",
        "    index = np.where(train_y == label)\n",
        "    data_x = train_x[index[0], 0]\n",
        "    data_y = train_x[index[0], 1]\n",
        "    c = color[index[0],:]\n",
        "    plt.scatter(data_x, data_y,  c=c, edgecolors='black')\n",
        "plt.axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAROaHGR-9vs"
      },
      "source": [
        "## Test data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t5FrMi2b-9vs"
      },
      "outputs": [],
      "source": [
        "nums_test = 100  # set the number of test samples \n",
        "\n",
        "test_m1 = np.array([0.5, -0.5]) ## mean of the 1-st Gaussian distribution\n",
        "test_m2 = np.array([-0.5, 0.5])  ## mean of the 2-nd Gaussian distribuion\n",
        "test_means = [test_m1, test_m2]\n",
        "\n",
        "print(\"The means of two Gaussian distribution:\", test_means[0], test_means[1])\n",
        "\n",
        "## Define the  covariance matrice of the two Gaussian distributions\n",
        "test_c1 = np.array([[0.5,0], [0,0.5]])\n",
        "test_c2 = np.array([[0.5, 0], [0, 0.5]])\n",
        "test_covs = [test_c1, test_c2] ## Each covariance matrix decides the shape of the Gaussian distribution\n",
        "print(\"The covariance matrices of the two Gaussian distributions:\\n\", test_covs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3RPbi2o-9vs"
      },
      "outputs": [],
      "source": [
        "test_data = sample_generation(test_means, test_covs, nums_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gql17MsM-9vs"
      },
      "outputs": [],
      "source": [
        "# Split testing data and testing label\n",
        "test_x, test_y = test_data[:, 0:2], test_data[:,2]\n",
        "test_y = test_y.reshape(nums_test, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6KM7hWX-9vt"
      },
      "outputs": [],
      "source": [
        "acc = comp_accuracy(test_x, theta_hat, test_y)\n",
        "\n",
        "print(\"The testing accuracy is {:.4f}\".format(acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3rrIXNB-9vt"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Tutorial_logistic_regression_2021.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.3 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "eb0701228c79383642a75a290a7b0e4ed993e1ad2c5e49dbdc4ab8afd54794cf"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
